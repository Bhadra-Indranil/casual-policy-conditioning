"""
Final Analysis Script for Policy Conditioning Study

Scientific Protocol Implementation:
1. Learning Efficiency (IQ Test) - Learning curves from training logs
2. Safety/CVaR (Crash Test) - Worst-case performance analysis  
3. Adaptation Speed (Reflex Test) - Settling time measurement

Generates:
- 1_learning_curve.png
- 2_safety_cvar.png
- 3_settling_time.png
"""

import os
import csv
import json
import warnings
from pathlib import Path
from collections import defaultdict
from dataclasses import dataclass, asdict
from typing import Dict, List, Tuple, Optional

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import FuncFormatter
import torch
from stable_baselines3 import PPO

from custom_lander import make_env, VelocityTrackingLander
import gymnasium as gym

warnings.filterwarnings('ignore')

# =============================================================================
# Configuration
# =============================================================================

EXPERIMENTS_DIR = Path(__file__).parent / "experiments"
OUTPUT_DIR = Path(__file__).parent / "evaluation_results"
OUTPUT_DIR.mkdir(exist_ok=True)

# Agent configuration
AGENT_TYPES = ['baseline', 'conditioned', 'noise', 'shuffled']
SEEDS = [42, 123, 456, 789, 1024]
LAMBDA = 0.5

# Evaluation parameters
N_EVAL_EPISODES = 50          # Episodes for CVaR evaluation
SETTLING_TOLERANCE = 0.30     # 30% - relaxed since agents prioritize landing over tracking
SETTLING_HOLD_STEPS = 10      # Reduced - just need brief stability
CVAR_ALPHA = 0.10             # Bottom 10% for CVaR

# Plotting style
COLORS = {
    'baseline': '#7f7f7f',    # Gray
    'conditioned': '#2ca02c', # Green
    'noise': '#ff7f0e',       # Orange
    'shuffled': '#9467bd',    # Purple
}
LABELS = {
    'baseline': 'Baseline (No Signal)',
    'conditioned': 'Conditioned (True Signal)',
    'noise': 'Noise (Random Signal)',
    'shuffled': 'Shuffled (Wrong Timing)',
}

plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.dpi'] = 150
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 11
plt.rcParams['axes.labelsize'] = 12
plt.rcParams['axes.titlesize'] = 14
plt.rcParams['legend.fontsize'] = 10


# =============================================================================
# Step 1: Learning Efficiency (IQ Test)
# =============================================================================

def parse_monitor_csv(csv_path: Path) -> List[dict]:
    """Parse a SB3 monitor CSV file."""
    episodes = []
    try:
        with open(csv_path, 'r') as f:
            first_line = f.readline()  # Skip metadata header
            reader = csv.DictReader(f)
            cumulative_steps = 0
            for row in reader:
                try:
                    length = int(row['l'])
                    cumulative_steps += length
                    episodes.append({
                        'reward': float(row['r']),
                        'length': length,
                        'time': float(row['t']),
                        'cumulative_steps': cumulative_steps
                    })
                except (KeyError, ValueError):
                    continue
    except Exception as e:
        print(f"Warning: Could not parse {csv_path}: {e}")
    return episodes


def find_best_model_path(agent_type: str, seed: int) -> Optional[Path]:
    """Find the best_model.zip for a given agent type and seed."""
    pattern = f"{agent_type}_lambda{LAMBDA}_seed{seed}"
    
    for run_dir in sorted(EXPERIMENTS_DIR.iterdir(), reverse=True):
        if not run_dir.is_dir() or not run_dir.name.startswith("run_"):
            continue
        
        for seed_dir in run_dir.iterdir():
            if seed_dir.is_dir() and seed_dir.name == pattern:
                model_path = seed_dir / "models" / "best_model.zip"
                if model_path.exists():
                    return model_path
    return None


def collect_learning_curves() -> Dict[str, Dict[int, List[dict]]]:
    """
    Collect learning curves from all monitor.csv files.
    Returns: {agent_type: {seed: [episodes]}}
    """
    print("\n" + "=" * 60)
    print("STEP 1: Collecting Learning Curves (IQ Test)")
    print("=" * 60)
    
    data = {agent: {} for agent in AGENT_TYPES}
    
    for agent_type in AGENT_TYPES:
        pattern = f"{agent_type}_lambda{LAMBDA}_seed"
        
        for run_dir in EXPERIMENTS_DIR.iterdir():
            if not run_dir.is_dir():
                continue
                
            for seed_dir in run_dir.iterdir():
                if not seed_dir.is_dir() or not seed_dir.name.startswith(pattern):
                    continue
                
                # Extract seed
                try:
                    seed = int(seed_dir.name.split('seed')[1])
                except:
                    continue
                
                # Skip if we already have this seed (keep latest/most complete)
                logs_dir = seed_dir / "logs"
                if not logs_dir.exists():
                    continue
                
                # Aggregate all monitor files
                all_episodes = []
                for monitor_file in logs_dir.glob("*.monitor.csv"):
                    all_episodes.extend(parse_monitor_csv(monitor_file))
                
                if all_episodes:
                    # Sort by cumulative steps and deduplicate
                    all_episodes.sort(key=lambda x: x['cumulative_steps'])
                    
                    # Keep the run with more episodes
                    if seed not in data[agent_type] or len(all_episodes) > len(data[agent_type][seed]):
                        data[agent_type][seed] = all_episodes
                        print(f"  ‚úì {agent_type} seed {seed}: {len(all_episodes)} episodes")
    
    return data


def interpolate_to_timesteps(episodes: List[dict], max_timesteps: int = 1_000_000, 
                              resolution: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
    """
    Interpolate episode rewards to fixed timestep grid.
    Returns: (timesteps, rewards)
    """
    timesteps = np.linspace(0, max_timesteps, resolution)
    rewards = np.zeros(resolution)
    
    if not episodes:
        return timesteps, rewards
    
    # Convert to cumulative timesteps and rewards
    ep_steps = np.array([ep['cumulative_steps'] for ep in episodes])
    ep_rewards = np.array([ep['reward'] for ep in episodes])
    
    # Use rolling average for smoother curves
    window = min(100, len(ep_rewards) // 10) if len(ep_rewards) > 10 else 1
    if window > 1:
        ep_rewards_smooth = np.convolve(ep_rewards, np.ones(window)/window, mode='valid')
        ep_steps_smooth = ep_steps[window-1:]
    else:
        ep_rewards_smooth = ep_rewards
        ep_steps_smooth = ep_steps
    
    # Interpolate
    rewards = np.interp(timesteps, ep_steps_smooth, ep_rewards_smooth)
    
    return timesteps, rewards


def plot_learning_curves(data: Dict[str, Dict[int, List[dict]]]):
    """
    Plot learning curves: Mean ¬± Std across seeds for each agent.
    Generates: 1_learning_curve.png
    """
    print("\nüìä Generating Learning Curve plot...")
    
    fig, ax = plt.subplots(figsize=(12, 7))
    
    max_timesteps = 1_000_000
    resolution = 500
    
    for agent_type in AGENT_TYPES:
        if agent_type not in data or not data[agent_type]:
            continue
        
        # Interpolate all seeds to same timestep grid
        all_rewards = []
        for seed, episodes in data[agent_type].items():
            timesteps, rewards = interpolate_to_timesteps(episodes, max_timesteps, resolution)
            all_rewards.append(rewards)
        
        if not all_rewards:
            continue
        
        reward_matrix = np.array(all_rewards)
        mean_reward = np.mean(reward_matrix, axis=0)
        std_reward = np.std(reward_matrix, axis=0)
        
        # Plot mean line
        ax.plot(timesteps, mean_reward, 
                color=COLORS[agent_type], 
                label=LABELS[agent_type],
                linewidth=2)
        
        # Plot uncertainty shadow
        ax.fill_between(timesteps, 
                        mean_reward - std_reward, 
                        mean_reward + std_reward,
                        color=COLORS[agent_type], 
                        alpha=0.2)
    
    ax.set_xlabel('Training Timesteps', fontsize=13)
    ax.set_ylabel('Episode Reward', fontsize=13)
    ax.set_title('Learning Efficiency: Policy Conditioning vs Control Groups', fontsize=15, fontweight='bold')
    
    # Format x-axis
    ax.xaxis.set_major_formatter(FuncFormatter(lambda x, p: f'{x/1e6:.1f}M' if x >= 1e6 else f'{x/1e3:.0f}K'))
    
    ax.legend(loc='lower right', framealpha=0.9)
    ax.set_xlim(0, max_timesteps)
    ax.grid(True, alpha=0.3)
    
    # Add annotation
    ax.text(0.02, 0.98, 'Higher = Better Learning', 
            transform=ax.transAxes, fontsize=10, 
            verticalalignment='top', style='italic', color='gray')
    
    plt.tight_layout()
    output_path = OUTPUT_DIR / "1_learning_curve.png"
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()
    print(f"  ‚úì Saved: {output_path}")
    
    return data


# =============================================================================
# Step 2: Safety / CVaR (Crash Test)
# =============================================================================

def load_model_safe(model_path: Path, agent_type: str, device: str = 'cpu') -> Optional[PPO]:
    """Load model by creating fresh PPO and loading only policy weights (bypasses pickle issues)."""
    import zipfile
    import io
    
    try:
        # Create fresh model with correct architecture
        env = make_env(agent_type=agent_type, lambda_penalty=LAMBDA)
        model = PPO('MlpPolicy', env, verbose=0, device=device)
        
        # Load only the policy weights (avoids pickle compatibility issues)
        with zipfile.ZipFile(model_path, 'r') as z:
            with z.open('policy.pth') as f:
                policy_state = torch.load(io.BytesIO(f.read()), map_location=device, weights_only=False)
                model.policy.load_state_dict(policy_state)
        
        env.close()
        return model
    except Exception as e:
        print(f"Load failed: {e}")
        return None


def run_cvar_evaluation(model: PPO, agent_type: str, n_episodes: int = N_EVAL_EPISODES) -> Dict:
    """
    Run evaluation episodes and calculate CVaR.
    """
    env = make_env(agent_type=agent_type, lambda_penalty=LAMBDA)
    
    episode_rewards = []
    
    for ep in range(n_episodes):
        obs, info = env.reset()
        total_reward = 0
        done = False
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            total_reward += reward
        
        episode_rewards.append(total_reward)
    
    env.close()
    
    rewards = np.array(episode_rewards)
    sorted_rewards = np.sort(rewards)
    
    # CVaR: Mean of worst Œ±%
    var_idx = max(1, int(np.floor(CVAR_ALPHA * len(sorted_rewards))))
    cvar = np.mean(sorted_rewards[:var_idx])
    var = sorted_rewards[var_idx - 1]
    
    return {
        'mean': np.mean(rewards),
        'std': np.std(rewards),
        'min': np.min(rewards),
        'max': np.max(rewards),
        'var': var,
        'cvar': cvar,
        'rewards': episode_rewards
    }


def collect_cvar_data() -> Dict[str, Dict[int, Dict]]:
    """
    Run CVaR evaluation for all agents.
    Returns: {agent_type: {seed: cvar_results}}
    """
    print("\n" + "=" * 60)
    print("STEP 2: Safety Analysis / CVaR (Crash Test)")
    print("=" * 60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    results = {agent: {} for agent in AGENT_TYPES}
    
    for agent_type in AGENT_TYPES:
        print(f"\nüìä Evaluating {agent_type}...")
        
        for seed in SEEDS:
            model_path = find_best_model_path(agent_type, seed)
            
            if model_path is None:
                print(f"  ‚ö† No model found for {agent_type} seed {seed}")
                continue
            
            print(f"  Loading seed {seed}...", end=" ", flush=True)
            model = load_model_safe(model_path, agent_type, device=device)
            
            if model is None:
                print("FAILED")
                continue
                
            print("OK, evaluating...", end=" ", flush=True)
            try:
                cvar_result = run_cvar_evaluation(model, agent_type)
                results[agent_type][seed] = cvar_result
                print(f"Mean={cvar_result['mean']:.1f}, CVaR={cvar_result['cvar']:.1f}")
            except Exception as e:
                print(f"Error: {e}")
    
    return results


def plot_cvar(data: Dict[str, Dict[int, Dict]]):
    """
    Plot CVaR comparison bar chart.
    Generates: 2_safety_cvar.png
    """
    print("\nüìä Generating CVaR plot...")
    
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # Aggregate data
    means = []
    stds = []
    cvars = []
    cvar_stds = []
    agents = []
    
    for agent_type in AGENT_TYPES:
        if agent_type not in data or not data[agent_type]:
            continue
        
        seed_means = [d['mean'] for d in data[agent_type].values()]
        seed_cvars = [d['cvar'] for d in data[agent_type].values()]
        
        agents.append(agent_type)
        means.append(np.mean(seed_means))
        stds.append(np.std(seed_means))
        cvars.append(np.mean(seed_cvars))
        cvar_stds.append(np.std(seed_cvars))
    
    x = np.arange(len(agents))
    width = 0.6
    
    # Plot 1: Mean Reward
    colors = [COLORS[a] for a in agents]
    bars1 = ax1.bar(x, means, width, yerr=stds, capsize=5, color=colors, edgecolor='black', linewidth=1)
    ax1.set_ylabel('Mean Episode Reward', fontsize=12)
    ax1.set_title('Average Performance', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels([LABELS[a].split('(')[0].strip() for a in agents], rotation=15, ha='right')
    ax1.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    
    # Add value labels
    for bar, mean in zip(bars1, means):
        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, 
                 f'{mean:.0f}', ha='center', va='bottom', fontsize=10)
    
    # Plot 2: CVaR (worst 10%)
    bars2 = ax2.bar(x, cvars, width, yerr=cvar_stds, capsize=5, color=colors, edgecolor='black', linewidth=1)
    ax2.set_ylabel(f'CVaR ({int(CVAR_ALPHA*100)}%) - Worst Case Reward', fontsize=12)
    ax2.set_title('Tail Risk: Worst 10% of Episodes', fontsize=14, fontweight='bold')
    ax2.set_xticks(x)
    ax2.set_xticklabels([LABELS[a].split('(')[0].strip() for a in agents], rotation=15, ha='right')
    ax2.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
    
    # Add value labels
    for bar, cvar in zip(bars2, cvars):
        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, 
                 f'{cvar:.0f}', ha='center', va='bottom', fontsize=10)
    
    # Add annotation
    ax2.text(0.02, 0.02, 'Higher = Safer (less negative)', 
             transform=ax2.transAxes, fontsize=10, 
             verticalalignment='bottom', style='italic', color='gray')
    
    plt.suptitle('Safety Analysis: Robustness to Worst-Case Scenarios', fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()
    
    output_path = OUTPUT_DIR / "2_safety_cvar.png"
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()
    print(f"  ‚úì Saved: {output_path}")
    
    return data


# =============================================================================
# Step 3: Adaptation Speed / Settling Time (Reflex Test)
# =============================================================================

def calculate_settling_time(velocity_trace: np.ndarray, target: float, 
                           start_idx: int, tolerance: float = SETTLING_TOLERANCE,
                           hold_steps: int = SETTLING_HOLD_STEPS) -> float:
    """
    Calculate settling time after a target change.
    Returns: Number of steps to settle (or NaN if never settled)
    """
    for t in range(start_idx, len(velocity_trace) - hold_steps):
        error = abs(velocity_trace[t] - target)
        tol = max(tolerance * abs(target), tolerance)  # Relative or absolute
        
        if error <= tol:
            # Check if stays within band
            future_errors = np.abs(velocity_trace[t:t+hold_steps] - target)
            if np.all(future_errors <= tol):
                return t - start_idx
    
    return np.nan


def run_settling_time_evaluation(model: PPO, agent_type: str, 
                                  n_episodes: int = 30,
                                  step_magnitude: float = 0.5) -> Dict:
    """
    Run step-response test: measure adaptation speed after velocity target change.
    
    Since agents prioritize landing over precise velocity tracking, we measure:
    1. Error reduction: How much does error decrease after target change?
    2. Response direction: Does agent move toward the new target?
    3. Adaptation rate: How quickly does error decrease?
    """
    base_env = gym.make("LunarLanderContinuous-v3")
    env = VelocityTrackingLander(
        base_env, 
        agent_type=agent_type,
        change_interval=150,  # Change at step 150
        lambda_penalty=LAMBDA
    )
    
    adaptation_scores = []
    error_reductions = []
    
    for ep in range(n_episodes):
        obs, info = env.reset()
        
        # Set up controlled step test: 0.0 ‚Üí step_magnitude
        v_initial = 0.0
        v_final = step_magnitude * (1 if ep % 2 == 0 else -1)  # Alternate direction
        
        c_initial = (v_initial - env.v_min) / env.v_range
        c_final = (v_final - env.v_min) / env.v_range
        
        env.target_schedule = [c_initial, c_final] + [c_final] * 10
        env.shuffled_schedule = env.target_schedule.copy()
        env.current_signal = c_initial
        env.target_vx = v_initial
        env.schedule_index = 0
        
        velocities = []
        targets = []
        change_idx = None
        
        done = False
        step = 0
        max_steps = 400
        
        while not done and step < max_steps:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            step += 1
            
            velocities.append(info['current_vx'])
            targets.append(info['target_vx'])
            
            if info.get('target_changed', False) and change_idx is None:
                change_idx = step
        
        # Measure adaptation after the change
        if change_idx and len(velocities) > change_idx + 100:
            velocities = np.array(velocities)
            
            # Error immediately after change (first 50 steps)
            early_error = np.mean(np.abs(velocities[change_idx:change_idx+50] - v_final))
            
            # Error later (steps 50-100 after change)
            late_error = np.mean(np.abs(velocities[change_idx+50:change_idx+100] - v_final))
            
            # Adaptation score: how much did error reduce?
            if early_error > 0:
                error_reduction = (early_error - late_error) / early_error
                error_reductions.append(error_reduction)
                adaptation_scores.append(late_error)  # Lower is better
    
    env.close()
    
    return {
        'mean': np.mean(adaptation_scores) if adaptation_scores else np.nan,  # Mean late error (lower = better)
        'std': np.std(adaptation_scores) if adaptation_scores else np.nan,
        'median': np.median(adaptation_scores) if adaptation_scores else np.nan,
        'error_reduction': np.mean(error_reductions) if error_reductions else np.nan,  # % error reduced
        'success_rate': len(adaptation_scores) / n_episodes,
        'all_scores': adaptation_scores
    }


def collect_settling_time_data() -> Dict[str, Dict[int, Dict]]:
    """
    Run settling time evaluation for all agents.
    Returns: {agent_type: {seed: settling_results}}
    """
    print("\n" + "=" * 60)
    print("STEP 3: Adaptation Speed / Settling Time (Reflex Test)")
    print("=" * 60)
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    results = {agent: {} for agent in AGENT_TYPES}
    
    for agent_type in AGENT_TYPES:
        print(f"\nüìä Evaluating {agent_type}...")
        
        for seed in SEEDS:
            model_path = find_best_model_path(agent_type, seed)
            
            if model_path is None:
                print(f"  ‚ö† No model found for {agent_type} seed {seed}")
                continue
            
            print(f"  Loading seed {seed}...", end=" ", flush=True)
            model = load_model_safe(model_path, agent_type, device=device)
            
            if model is None:
                print("FAILED")
                continue
            
            print("OK, evaluating...", end=" ", flush=True)
            try:
                settling_result = run_settling_time_evaluation(model, agent_type)
                results[agent_type][seed] = settling_result
                
                if not np.isnan(settling_result['mean']):
                    print(f"Error={settling_result['mean']:.3f}, Reduction={settling_result['error_reduction']:.1%}")
                else:
                    print("No valid episodes")
            except Exception as e:
                print(f"Error: {e}")
    
    return results


def plot_settling_time(data: Dict[str, Dict[int, Dict]]):
    """
    Plot settling time comparison bar chart.
    Generates: 3_settling_time.png
    """
    print("\nüìä Generating Settling Time plot...")
    
    fig, ax = plt.subplots(figsize=(10, 7))
    
    # Aggregate data
    means = []
    stds = []
    agents = []
    success_rates = []
    
    for agent_type in AGENT_TYPES:
        if agent_type not in data or not data[agent_type]:
            continue
        
        seed_means = [d['mean'] for d in data[agent_type].values() if not np.isnan(d['mean'])]
        seed_rates = [d['success_rate'] for d in data[agent_type].values()]
        
        if seed_means:
            agents.append(agent_type)
            means.append(np.mean(seed_means))
            stds.append(np.std(seed_means))
            success_rates.append(np.mean(seed_rates))
    
    x = np.arange(len(agents))
    width = 0.6
    
    colors = [COLORS[a] for a in agents]
    bars = ax.bar(x, means, width, yerr=stds, capsize=5, color=colors, edgecolor='black', linewidth=1)
    
    ax.set_ylabel('Tracking Error After Adaptation', fontsize=13)
    ax.set_xlabel('Agent Type', fontsize=13)
    ax.set_title('Adaptation Quality: Velocity Tracking Error After Target Change', fontsize=15, fontweight='bold')
    ax.set_xticks(x)
    ax.set_xticklabels([LABELS[a].split('(')[0].strip() for a in agents], rotation=15, ha='right')
    
    # Add value labels and error reduction rates
    for bar, mean, rate in zip(bars, means, success_rates):
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, 
                f'{mean:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')
        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() / 2, 
                f'{rate:.0%}\nepisodes', ha='center', va='center', fontsize=9, color='white')
    
    # Add annotation
    ax.text(0.98, 0.98, 'Lower = Better Tracking ‚úì', 
            transform=ax.transAxes, fontsize=10, 
            ha='right', va='top', style='italic', color='gray')
    
    plt.tight_layout()
    
    output_path = OUTPUT_DIR / "3_settling_time.png"
    plt.savefig(output_path, bbox_inches='tight')
    plt.close()
    print(f"  ‚úì Saved: {output_path}")
    
    return data


# =============================================================================
# Step 4: Causal Verification Summary
# =============================================================================

def print_causal_verification(learning_data, cvar_data, settling_data):
    """
    Print the Lie Detector analysis: Conditioned vs Shuffled comparison.
    """
    print("\n" + "=" * 60)
    print("STEP 4: Causal Verification (Lie Detector)")
    print("=" * 60)
    print("\nComparing CONDITIONED vs SHUFFLED agents:")
    print("(Both saw same signal values, but Shuffled saw wrong timing)\n")
    
    results = []
    
    # Learning efficiency
    if 'conditioned' in learning_data and 'shuffled' in learning_data:
        cond_final = []
        shuf_final = []
        for seed, eps in learning_data['conditioned'].items():
            if eps:
                final_rewards = [e['reward'] for e in eps[-100:]]
                cond_final.append(np.mean(final_rewards))
        for seed, eps in learning_data['shuffled'].items():
            if eps:
                final_rewards = [e['reward'] for e in eps[-100:]]
                shuf_final.append(np.mean(final_rewards))
        
        if cond_final and shuf_final:
            cond_mean = np.mean(cond_final)
            shuf_mean = np.mean(shuf_final)
            diff = cond_mean - shuf_mean
            winner = "‚úì CONDITIONED" if diff > 0 else "‚úó SHUFFLED"
            results.append(('Final Reward', cond_mean, shuf_mean, diff, winner))
            print(f"  Final Reward:     Conditioned={cond_mean:.1f}  vs  Shuffled={shuf_mean:.1f}  ‚Üí {winner} (+{abs(diff):.1f})")
    
    # CVaR
    if 'conditioned' in cvar_data and 'shuffled' in cvar_data:
        cond_cvar = [d['cvar'] for d in cvar_data['conditioned'].values()]
        shuf_cvar = [d['cvar'] for d in cvar_data['shuffled'].values()]
        
        if cond_cvar and shuf_cvar:
            cond_mean = np.mean(cond_cvar)
            shuf_mean = np.mean(shuf_cvar)
            diff = cond_mean - shuf_mean
            winner = "‚úì CONDITIONED" if diff > 0 else "‚úó SHUFFLED"
            results.append(('CVaR (Safety)', cond_mean, shuf_mean, diff, winner))
            print(f"  CVaR (Safety):    Conditioned={cond_mean:.1f}  vs  Shuffled={shuf_mean:.1f}  ‚Üí {winner} (+{abs(diff):.1f})")
    
    # Adaptation (tracking error - lower is better)
    if 'conditioned' in settling_data and 'shuffled' in settling_data:
        cond_err = [d['mean'] for d in settling_data['conditioned'].values() if not np.isnan(d['mean'])]
        shuf_err = [d['mean'] for d in settling_data['shuffled'].values() if not np.isnan(d['mean'])]
        
        if cond_err and shuf_err:
            cond_mean = np.mean(cond_err)
            shuf_mean = np.mean(shuf_err)
            diff = shuf_mean - cond_mean  # Lower error is better, so positive diff = conditioned wins
            winner = "‚úì CONDITIONED" if diff > 0 else "‚úó SHUFFLED"
            results.append(('Tracking Error', cond_mean, shuf_mean, diff, winner))
            print(f"  Tracking Error:   Conditioned={cond_mean:.3f}  vs  Shuffled={shuf_mean:.3f}  ‚Üí {winner} ({abs(diff):.3f} lower)")
    
    # Verdict
    cond_wins = sum(1 for r in results if 'CONDITIONED' in r[4])
    print(f"\n{'='*40}")
    if cond_wins == len(results):
        print("‚úÖ CAUSAL VERIFICATION PASSED")
        print("   The TIMING of the signal matters, not just its values.")
        print("   This validates the paper's central hypothesis.")
    elif cond_wins > len(results) // 2:
        print("‚ö†Ô∏è  PARTIAL VERIFICATION")
        print(f"   Conditioned wins {cond_wins}/{len(results)} metrics.")
    else:
        print("‚ùå VERIFICATION FAILED")
        print("   Shuffled performed comparably or better.")
    print(f"{'='*40}")
    
    return results


# =============================================================================
# Main
# =============================================================================

def save_results_json(learning_data, cvar_data, settling_data):
    """Save all numerical results to JSON for the paper."""
    results = {
        'learning_final_100': {},
        'cvar': {},
        'settling_time': {}
    }
    
    for agent in AGENT_TYPES:
        if agent in learning_data:
            final_rewards = []
            for seed, eps in learning_data[agent].items():
                if eps:
                    final_rewards.append(np.mean([e['reward'] for e in eps[-100:]]))
            if final_rewards:
                results['learning_final_100'][agent] = {
                    'mean': float(np.mean(final_rewards)),
                    'std': float(np.std(final_rewards)),
                    'seeds': len(final_rewards)
                }
        
        if agent in cvar_data:
            cvars = [d['cvar'] for d in cvar_data[agent].values()]
            means = [d['mean'] for d in cvar_data[agent].values()]
            if cvars:
                results['cvar'][agent] = {
                    'mean_reward': float(np.mean(means)),
                    'mean_cvar': float(np.mean(cvars)),
                    'std_cvar': float(np.std(cvars)),
                    'seeds': len(cvars)
                }
        
        if agent in settling_data:
            times = [d['mean'] for d in settling_data[agent].values() if not np.isnan(d['mean'])]
            rates = [d['success_rate'] for d in settling_data[agent].values()]
            if times:
                results['settling_time'][agent] = {
                    'mean': float(np.mean(times)),
                    'std': float(np.std(times)),
                    'success_rate': float(np.mean(rates)),
                    'seeds': len(times)
                }
    
    output_path = OUTPUT_DIR / "analysis_results.json"
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\n‚úì Saved numerical results: {output_path}")


def main():
    print("\n" + "=" * 60)
    print("FINAL ANALYSIS: Policy Conditioning Study")
    print("=" * 60)
    print(f"Output directory: {OUTPUT_DIR}")
    
    # Step 1: Learning Curves
    learning_data = collect_learning_curves()
    plot_learning_curves(learning_data)
    
    # Step 2: CVaR (Safety)
    cvar_data = collect_cvar_data()
    plot_cvar(cvar_data)
    
    # Step 3: Settling Time (Adaptation)
    settling_data = collect_settling_time_data()
    plot_settling_time(settling_data)
    
    # Step 4: Causal Verification
    print_causal_verification(learning_data, cvar_data, settling_data)
    
    # Save results
    save_results_json(learning_data, cvar_data, settling_data)
    
    print("\n" + "=" * 60)
    print("ANALYSIS COMPLETE")
    print("=" * 60)
    print(f"\nGenerated files in {OUTPUT_DIR}:")
    print("  üìä 1_learning_curve.png  - Learning efficiency over time")
    print("  üìä 2_safety_cvar.png     - Tail risk / worst-case analysis")
    print("  üìä 3_settling_time.png   - Adaptation speed comparison")
    print("  üìÑ analysis_results.json - Numerical results for paper")
    print()


if __name__ == "__main__":
    main()
